{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTasdIKM9OVV",
        "outputId": "797ae028-949b-4fdf-8bdf-929cfdc4e1fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertForTokenClassification\n"
      ],
      "metadata": {
        "id": "1DFJZPEi-COp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.read_csv('/content/drive/MyDrive/MSC - research/df_regression_20240714.csv');\n",
        "\n",
        "df = pd.DataFrame(columns=['texts', 'aspects', 'sentiments'])\n",
        "\n",
        "df['texts'] = df_all['tokens'].apply(ast.literal_eval)\n",
        "df['aspects'] = df_all['bio_tags'].apply(ast.literal_eval)\n",
        "df['sentiments'] = df_all['sentiment_tags'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "PwDJSu28EH6G"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the mapping for aspects\n",
        "aspect_mapping = {\n",
        "    'O': 'O',\n",
        "    'B-overall': 'overall',\n",
        "    'B-length': 'length',\n",
        "    'B-hip': 'hip',\n",
        "    'B-buttoned': 'buttoned',\n",
        "    'B-neck': 'neck',\n",
        "    'B-sleeve': 'sleeve',\n",
        "    'B-chest': 'chest',\n",
        "    'B-waist': 'waist',\n",
        "    'B-shoulder': 'shoulder',\n",
        "    'B-length_indirect': 'length',\n",
        "    'B-buttoned_indirect': 'buttoned',\n",
        "    'B-sleeve_indirect': 'sleeve',\n",
        "    'B-overall_indirect': 'overall',\n",
        "    'I-overall': 'overall',\n",
        "    'I-length': 'length',\n",
        "    'I-hip': 'hip',\n",
        "    'I-buttoned': 'buttoned',\n",
        "    'I-neck': 'neck',\n",
        "    'I-sleeve': 'sleeve',\n",
        "    'I-chest': 'chest',\n",
        "    'I-waist': 'waist',\n",
        "    'I-shoulder': 'shoulder',\n",
        "    'I-length_indirect': 'length',\n",
        "    'I-buttoned_indirect': 'buttoned',\n",
        "    'I-sleeve_indirect': 'sleeve',\n",
        "    'I-overall_indirect': 'overall',\n",
        "}\n",
        "\n",
        "# Define the mapping for sentiments\n",
        "sentiment_mapping = {\n",
        "    'O': 'O',\n",
        "    'B-negative': 'negative',\n",
        "    'I-negative': 'negative',\n",
        "    'B-neutral': 'neutral',\n",
        "    'I-neutral': 'neutral',\n",
        "    'B-positive': 'positive',\n",
        "    'I-positive': 'positive',\n",
        "}\n",
        "\n",
        "# Function to apply mappings\n",
        "def apply_mapping(value, mapping):\n",
        "    return [mapping.get(item, 'out') for item in value]\n",
        "\n",
        "# Update the DataFrame\n",
        "df['aspects'] = df['aspects'].apply(apply_mapping, args=(aspect_mapping,))\n",
        "df['sentiments'] = df['sentiments'].apply(apply_mapping, args=(sentiment_mapping,))\n"
      ],
      "metadata": {
        "id": "CwE_EiRKRxJM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(column):\n",
        "  # Extract distinct values from the lists\n",
        "  distinct_values = set()\n",
        "  df[column].apply(lambda x: distinct_values.update(x))\n",
        "\n",
        "  # Convert the set to a list if you need a list format\n",
        "  distinct_values_list = list(distinct_values)\n",
        "  return distinct_values_list\n"
      ],
      "metadata": {
        "id": "9rZQzyyffwlU"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aspect_count = len(get_labels('aspects'))\n",
        "sentiment_count = len(get_labels('sentiments'))\n",
        "max_length = df['texts'].apply(len).max()\n",
        "out_of_aspect_id = -100"
      ],
      "metadata": {
        "id": "U3X96l5Dfs3f"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_length_frequencies(column):\n",
        "    lengths = df[column].apply(len)\n",
        "    return lengths.value_counts()\n",
        "\n",
        "# Get frequencies for each column\n",
        "tokens_length_freq = get_length_frequencies('texts')\n",
        "aspect_length_freq = get_length_frequencies('aspects')\n",
        "sentiment_length_freq = get_length_frequencies('sentiments')\n",
        "\n",
        "# Display the frequencies\n",
        "print(\"Tokens Length Frequencies:\\n\", tokens_length_freq)\n",
        "print(\"\\nAspect Length Frequencies:\\n\", aspect_length_freq)\n",
        "print(\"\\nSentiment Length Frequencies:\\n\", sentiment_length_freq)\n",
        "\n",
        "\n",
        "\n",
        "# Filter rows where the length of lists in 'tokens' column is 15\n",
        "df = df[df['texts'].apply(len) == 15]\n",
        "\n",
        "# View the filtered DataFrame\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhFkf3AOwUx9",
        "outputId": "c467c11a-3eec-43ba-a06b-683ae21922bd"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens Length Frequencies:\n",
            " texts\n",
            "15    11\n",
            "19     9\n",
            "14     9\n",
            "21     9\n",
            "10     8\n",
            "22     6\n",
            "18     6\n",
            "20     6\n",
            "12     6\n",
            "26     5\n",
            "23     5\n",
            "17     5\n",
            "13     5\n",
            "16     4\n",
            "30     4\n",
            "27     4\n",
            "9      4\n",
            "28     4\n",
            "8      4\n",
            "24     4\n",
            "5      3\n",
            "36     3\n",
            "7      3\n",
            "11     3\n",
            "38     3\n",
            "6      3\n",
            "41     3\n",
            "25     3\n",
            "31     2\n",
            "32     2\n",
            "46     2\n",
            "33     2\n",
            "40     2\n",
            "29     2\n",
            "52     1\n",
            "4      1\n",
            "55     1\n",
            "48     1\n",
            "56     1\n",
            "34     1\n",
            "61     1\n",
            "37     1\n",
            "47     1\n",
            "53     1\n",
            "57     1\n",
            "51     1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Aspect Length Frequencies:\n",
            " aspects\n",
            "15    11\n",
            "19     9\n",
            "14     9\n",
            "21     9\n",
            "10     8\n",
            "22     6\n",
            "18     6\n",
            "20     6\n",
            "12     6\n",
            "26     5\n",
            "23     5\n",
            "17     5\n",
            "13     5\n",
            "16     4\n",
            "30     4\n",
            "27     4\n",
            "9      4\n",
            "28     4\n",
            "8      4\n",
            "24     4\n",
            "5      3\n",
            "36     3\n",
            "7      3\n",
            "11     3\n",
            "38     3\n",
            "6      3\n",
            "41     3\n",
            "25     3\n",
            "31     2\n",
            "32     2\n",
            "46     2\n",
            "33     2\n",
            "40     2\n",
            "29     2\n",
            "52     1\n",
            "4      1\n",
            "55     1\n",
            "48     1\n",
            "56     1\n",
            "34     1\n",
            "61     1\n",
            "37     1\n",
            "47     1\n",
            "53     1\n",
            "57     1\n",
            "51     1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sentiment Length Frequencies:\n",
            " sentiments\n",
            "15    11\n",
            "19     9\n",
            "14     9\n",
            "21     9\n",
            "10     8\n",
            "22     6\n",
            "18     6\n",
            "20     6\n",
            "12     6\n",
            "26     5\n",
            "23     5\n",
            "17     5\n",
            "13     5\n",
            "16     4\n",
            "30     4\n",
            "27     4\n",
            "9      4\n",
            "28     4\n",
            "8      4\n",
            "24     4\n",
            "5      3\n",
            "36     3\n",
            "7      3\n",
            "11     3\n",
            "38     3\n",
            "6      3\n",
            "41     3\n",
            "25     3\n",
            "31     2\n",
            "32     2\n",
            "46     2\n",
            "33     2\n",
            "40     2\n",
            "29     2\n",
            "52     1\n",
            "4      1\n",
            "55     1\n",
            "48     1\n",
            "56     1\n",
            "34     1\n",
            "61     1\n",
            "37     1\n",
            "47     1\n",
            "53     1\n",
            "57     1\n",
            "51     1\n",
            "Name: count, dtype: int64\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, df, max_len=max_length):\n",
        "        self.df = df\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.texts = df['texts']\n",
        "        self.aspects = df['aspects']\n",
        "        self.sentiments = df['sentiments']\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        aspect_labels = self.aspects[idx]\n",
        "        sentiment_labels = self.sentiments[idx]\n",
        "\n",
        "        encoding = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_len)\n",
        "\n",
        "        # Convert labels to tensor and handle 'O' as ignore index for loss calculation\n",
        "        aspect_labels = torch.tensor([label_to_id(aspect) for aspect in aspect_labels], dtype=torch.long)\n",
        "        sentiment_labels = torch.tensor([sentiment_to_id(sentiment) for sentiment in sentiment_labels], dtype=torch.long)\n",
        "\n",
        "        return encoding, aspect_labels, sentiment_labels\n",
        "\n",
        "def label_to_id(label):\n",
        "    # label_dict = {'O': -100, 'B-overall': 0, 'B-length_indirect': 1, 'B-hip': 2, 'B-buttoned_indirect': 3, 'B-neck': 4,\n",
        "    #               'B-sleeve': 5, 'B-sleeve_indirect': 6, 'B-buttoned': 7, 'B-shoulder': 8, 'B-overall_indirect': 9,\n",
        "    #               'B-length': 10, 'B-chest': 11, 'B-waist': 12}\n",
        "    label_dict = {'O': out_of_aspect_id, 'overall': 0, 'length': 1, 'hip': 2, 'buttoned': 3, 'neck': 4,\n",
        "                  'sleeve': 5, 'shoulder': 6,'chest': 6, 'waist': 8}\n",
        "    return label_dict.get(label, out_of_aspect_id)\n",
        "\n",
        "def sentiment_to_id(sentiment):\n",
        "    # sentiment_dict = {'O': -100, 'B-negative': 0, 'B-neutral': 1, 'I-negative': 2, 'B-positive': 3, 'I-positive': 4}\n",
        "    sentiment_dict = {'O': out_of_aspect_id, 'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "    return sentiment_dict.get(sentiment, out_of_aspect_id)\n"
      ],
      "metadata": {
        "id": "79snZOcDQMj5"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    tokens = [item[0] for item in batch]\n",
        "    aspect_labels = [item[1] for item in batch]\n",
        "    sentiment_labels = [item[2] for item in batch]\n",
        "\n",
        "    # Pad tokens\n",
        "    tokens_padded = torch.nn.utils.rnn.pad_sequence([item['input_ids'].squeeze() for item in tokens], batch_first=True, padding_value=0)\n",
        "    attention_mask_padded = torch.nn.utils.rnn.pad_sequence([item['attention_mask'].squeeze() for item in tokens], batch_first=True, padding_value=0)\n",
        "\n",
        "    # Pad labels\n",
        "    aspect_labels_padded = torch.nn.utils.rnn.pad_sequence(aspect_labels, batch_first=True, padding_value=out_of_aspect_id)\n",
        "    sentiment_labels_padded = torch.nn.utils.rnn.pad_sequence(sentiment_labels, batch_first=True, padding_value=out_of_aspect_id)\n",
        "\n",
        "    # Reconstruct the tokens dictionary with padded sequences\n",
        "    tokens_padded_dict = {\n",
        "        'input_ids': tokens_padded,\n",
        "        'attention_mask': attention_mask_padded\n",
        "    }\n",
        "\n",
        "    return tokens_padded_dict, aspect_labels_padded, sentiment_labels_padded\n"
      ],
      "metadata": {
        "id": "alXR-Y7gpW5u"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseModel:\n",
        "    def __init__(self, model_name='bert-base-uncased', num_labels=1):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs=3, lr=1e-5):\n",
        "      return\n",
        "\n",
        "    def evaluate(self, val_loader):\n",
        "      return 1,1\n",
        "\n",
        "    def compute_loss_and_predictions(self, tokens, labels):\n",
        "        outputs = self.model(**tokens, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        return loss, preds\n",
        "\n",
        "    def compute_accuracy(self, preds, labels):\n",
        "        correct = (preds == labels).sum().item()\n",
        "        total = labels.numel()\n",
        "        accuracy = correct / total\n",
        "        return accuracy\n",
        "\n",
        "    def plot_training_and_validation(self, train_losses, train_accuracies, val_losses, val_accuracies):\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_losses, label='Training Loss')\n",
        "        plt.plot(val_losses, label='Validation Loss')\n",
        "        plt.title('Training & Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(train_accuracies, label='Training Accuracy')\n",
        "        plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "        plt.title('Training & Validation Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def test(self, test_loader):\n",
        "        _, accuracy = self.evaluate(test_loader)\n",
        "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sc_Ll4BxkDHt"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AspectModel(BaseModel):\n",
        "    def __init__(self, model_name='bert-base-uncased', num_labels=aspect_count):\n",
        "        super().__init__(model_name, num_labels)\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs=3, lr=1e-5):\n",
        "        self.model.train()\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "        train_losses, train_accuracies = [], []\n",
        "        val_losses, val_accuracies = [], []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for tokens, aspect_labels, _ in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                loss, preds = self.compute_loss_and_predictions(tokens, aspect_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                accuracy = self.compute_accuracy(preds, aspect_labels)\n",
        "                train_losses.append(loss.item())\n",
        "                train_accuracies.append(accuracy)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}: Train Loss = {train_losses[-1]:.4f}, Train Accuracy = {train_accuracies[-1]:.4f}\")\n",
        "            val_loss, val_accuracy = self.evaluate(val_loader)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "\n",
        "        self.plot_training_and_validation(train_losses, train_accuracies, val_losses, val_accuracies)\n",
        "\n",
        "    def evaluate(self, val_loader):\n",
        "        self.model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for tokens, aspect_labels, _ in val_loader:\n",
        "                loss, preds = self.compute_loss_and_predictions(tokens, aspect_labels)\n",
        "                val_loss += loss.item()\n",
        "                accuracy = self.compute_accuracy(preds, aspect_labels)\n",
        "                val_total += aspect_labels.numel()\n",
        "                all_preds.extend(preds.view(-1).tolist())\n",
        "                all_labels.extend(aspect_labels.view(-1).tolist())\n",
        "\n",
        "        val_avg_loss = val_loss / len(val_loader)\n",
        "        print(\"Evaluation Classification Report:\")\n",
        "        print(classification_report(all_labels, all_preds, labels=list(range(self.num_labels)), zero_division=0))\n",
        "        return val_avg_loss, accuracy"
      ],
      "metadata": {
        "id": "7iROFnA-kkir"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentModel(BaseModel):\n",
        "    def __init__(self, model_name='bert-base-uncased', num_labels=sentiment_count):  # Typically, sentiment analysis has fewer labels\n",
        "        super().__init__(model_name, num_labels)\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs=3, lr=1e-5):\n",
        "        self.model.train()\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "        train_losses, train_accuracies = [], []\n",
        "        val_losses, val_accuracies = [], []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for tokens, _, sentiment_labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                loss, preds = self.compute_loss_and_predictions(tokens, sentiment_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                accuracy = self.compute_accuracy(preds, sentiment_labels)\n",
        "                train_losses.append(loss.item())\n",
        "                train_accuracies.append(accuracy)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}: Train Loss = {train_losses[-1]:.4f}, Train Accuracy = {train_accuracies[-1]:.4f}\")\n",
        "            val_loss, val_accuracy = self.evaluate(val_loader)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "\n",
        "        self.plot_training_and_validation(train_losses, train_accuracies, val_losses, val_accuracies)\n",
        "\n",
        "    def evaluate(self, val_loader):\n",
        "        self.model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for tokens, _, sentiment_labels in val_loader:\n",
        "                loss, preds = self.compute_loss_and_predictions(tokens, sentiment_labels)\n",
        "                val_loss += loss.item()\n",
        "                accuracy = self.compute_accuracy(preds, sentiment_labels)\n",
        "                val_total += sentiment_labels.numel()\n",
        "                all_preds.extend(preds.view(-1).tolist())\n",
        "                all_labels.extend(sentiment_labels.view(-1).tolist())\n",
        "\n",
        "        val_avg_loss = val_loss / len(val_loader)\n",
        "        print(\"Evaluation Classification Report:\")\n",
        "        print(classification_report(all_labels, all_preds, labels=list(range(self.num_labels)), zero_division=0))\n",
        "        return val_avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "9NhfKqgafjM1"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset, val_split=0.4, test_split=0.4):\n",
        "    indices = list(range(len(dataset)))\n",
        "\n",
        "    # Split into train+validation and test sets\n",
        "    train_val_indices, test_indices = train_test_split(indices, test_size=test_split, random_state=42)\n",
        "\n",
        "    # Split train+validation set into train and validation sets\n",
        "    train_indices, val_indices = train_test_split(train_val_indices, test_size=val_split, random_state=42)\n",
        "\n",
        "    train_set = Subset(dataset, train_indices)\n",
        "    val_set = Subset(dataset, val_indices)\n",
        "    test_set = Subset(dataset, test_indices)\n",
        "\n",
        "    return train_set, val_set, test_set\n"
      ],
      "metadata": {
        "id": "zpfD2EcIisv1"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming ABSADataset is defined as in the previous messages\n",
        "dataset = ABSADataset(df)\n",
        "# loader = DataLoader(dataset, batch_size=8, shuffle=True) # check what batch_size is. 32 seems to be defuat\n",
        "\n",
        "train_set, val_set, test_set = split_dataset(dataset)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_set, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_set, batch_size=16, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "EtJCRGqQg_fw"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "aspect_model = AspectModel()\n",
        "sentiment_model = SentimentModel()\n",
        "\n",
        "# Example training setup, remember to define appropriate train, validate, and test datasets\n",
        "# Using the same loader for demonstration; in practice, you should separate these\n",
        "aspect_model.train(train_loader, val_loader, epochs=100, lr=1e-5)\n",
        "# sentiment_model.train(train_loader, val_loader, epochs=100, lr=1e-5)\n",
        "\n",
        "\n",
        "aspect_model.test(test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "pWQZEmLNmZFS",
        "outputId": "53c6e46e-4805-4c4c-f6c4-5e0986bfe13a"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "4",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 4",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-1fa7bf23fcc0>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Example training setup, remember to define appropriate train, validate, and test datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Using the same loader for demonstration; in practice, you should separate these\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0maspect_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# sentiment_model.train(train_loader, val_loader, epochs=100, lr=1e-5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-90-13d6f03ce163>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, val_loader, epochs, lr)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_and_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-d9eb50687f1b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0maspect_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maspects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msentiment_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 4"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class AspectModel:\n",
        "#     def __init__(self, model_name='bert-base-uncased', num_labels=aspect_count):\n",
        "#         self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "#         self.model = BertForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "#     def train(self, train_loader, val_loader, epochs=3, lr=1e-5):\n",
        "#         self.model.train()\n",
        "#         optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "#         train_losses, train_accuracies = [], []\n",
        "#         val_losses, val_accuracies = [], []\n",
        "\n",
        "#         for epoch in range(epochs):\n",
        "#             for tokens, aspect_labels, _ in train_loader:\n",
        "#                 optimizer.zero_grad()\n",
        "#                 loss, preds = self.compute_loss_and_predictions(tokens, aspect_labels)\n",
        "#                 loss.backward()\n",
        "#                 optimizer.step()\n",
        "\n",
        "#                 accuracy = self.compute_accuracy(preds, aspect_labels)\n",
        "#                 train_losses.append(loss.item())\n",
        "#                 train_accuracies.append(accuracy)\n",
        "\n",
        "#             print(f\"Epoch {epoch+1}: Train Loss = {train_losses[-1]:.4f}, Train Accuracy = {train_accuracies[-1]:.4f}\")\n",
        "#             val_loss, val_accuracy = self.evaluate(val_loader)\n",
        "#             val_losses.append(val_loss)\n",
        "#             val_accuracies.append(val_accuracy)\n",
        "\n",
        "#         self.plot_training_and_validation(train_losses, train_accuracies, val_losses, val_accuracies)\n",
        "\n",
        "#     def evaluate(self, val_loader):\n",
        "#         self.model.eval()\n",
        "#         val_loss, val_correct, val_total = 0, 0, 0\n",
        "#         all_preds, all_labels = [], []\n",
        "#         with torch.no_grad():\n",
        "#             for tokens, aspect_labels, _ in val_loader:\n",
        "#                 loss, preds = self.compute_loss_and_predictions(tokens, aspect_labels)\n",
        "#                 val_loss += loss.item()\n",
        "#                 accuracy = self.compute_accuracy(preds, aspect_labels)\n",
        "#                 val_total += aspect_labels.numel()\n",
        "#                 all_preds.extend(preds.view(-1).tolist())\n",
        "#                 all_labels.extend(aspect_labels.view(-1).tolist())\n",
        "\n",
        "#         val_avg_loss = val_loss / len(val_loader)\n",
        "#         print(\"Evaluation Classification Report:\")\n",
        "#         print(classification_report(all_labels, all_preds, labels=list(range(self.num_labels)), zero_division=0))\n",
        "#         return val_avg_loss, accuracy\n",
        "\n",
        "#     def compute_loss_and_predictions(self, tokens, labels):\n",
        "#         outputs = self.model(**tokens, labels=labels)\n",
        "#         loss = outputs.loss\n",
        "#         logits = outputs.logits\n",
        "#         preds = torch.argmax(logits, dim=-1)\n",
        "#         return loss, preds\n",
        "\n",
        "#     def compute_accuracy(self, preds, labels):\n",
        "#         correct = (preds == labels).sum().item()\n",
        "#         total = labels.numel()\n",
        "#         accuracy = correct / total\n",
        "#         return accuracy\n",
        "\n",
        "#     def plot_training_and_validation(self, train_losses, train_accuracies, val_losses, val_accuracies):\n",
        "#         plt.figure(figsize=(12, 6))\n",
        "#         plt.subplot(1, 2, 1)\n",
        "#         plt.plot(train_losses, label='Training Loss')\n",
        "#         plt.plot(val_losses, label='Validation Loss')\n",
        "#         plt.title('Training & Validation Loss')\n",
        "#         plt.xlabel('Epoch')\n",
        "#         plt.ylabel('Loss')\n",
        "#         plt.legend()\n",
        "\n",
        "#         plt.subplot(1, 2, 2)\n",
        "#         plt.plot(train_accuracies, label='Training Accuracy')\n",
        "#         plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "#         plt.title('Training & Validation Accuracy')\n",
        "#         plt.xlabel('Epoch')\n",
        "#         plt.ylabel('Accuracy')\n",
        "#         plt.legend()\n",
        "#         plt.show()\n",
        "\n",
        "#     def test(self, test_loader):\n",
        "#         _, accuracy = self.evaluate(test_loader)\n",
        "#         print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LA3Zs8X1Zx07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class AspectSentimentDataset(Dataset):\n",
        "#     def __init__(self, df):\n",
        "#         self.df = df\n",
        "#         self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = self.df.iloc[idx]\n",
        "#         encoding = self.tokenizer(\n",
        "#             item['tokens'],\n",
        "#             max_length=max_length,\n",
        "#             truncation=True,\n",
        "#             padding='max_length',\n",
        "#             return_tensors=\"pt\"\n",
        "#         )\n",
        "#         return {\n",
        "#             'input_ids': encoding['input_ids'].flatten(),\n",
        "#             'attention_mask': encoding['attention_mask'].flatten(),\n",
        "#             'aspect_labels': torch.tensor(item['aspect_labels']),\n",
        "#             'sentiment_labels': torch.tensor(item['sentiment_labels'])\n",
        "#         }"
      ],
      "metadata": {
        "id": "vK_O1y-wGTUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class AspectExtractor:\n",
        "#     def __init__(self, num_labels):\n",
        "#         self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#         self.model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "#         self.model.train()\n",
        "\n",
        "#     def train(self, dataloader, device, epochs=3, lr=2e-5):\n",
        "#         optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "#         self.model.to(device)\n",
        "#         predictions, true_labels = [], []\n",
        "#         for epoch in range(epochs):\n",
        "#             total_loss = 0\n",
        "#             for batch in tqdm(dataloader):\n",
        "#                 self.model.train()\n",
        "#                 input_ids = batch['input_ids'].to(device)\n",
        "#                 attention_mask = batch['attention_mask'].to(device)\n",
        "#                 labels = batch['aspect_labels'].to(device)\n",
        "\n",
        "#                 outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#                 loss = outputs.loss\n",
        "#                 optimizer.zero_grad()\n",
        "#                 loss.backward()\n",
        "#                 optimizer.step()\n",
        "#                 total_loss += loss.item()\n",
        "#                 logits = outputs.logits.argmax(dim=-1)\n",
        "#                 predictions.extend(logits.cpu().numpy())\n",
        "#                 true_labels.extend(labels.cpu().numpy())\n",
        "#             print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}\")\n",
        "\n",
        "#         print(\"Train - Classification Report:\")\n",
        "#         print(classification_report(true_labels, predictions))\n",
        "#         return total_loss / len(dataloader)\n",
        "\n",
        "#     def evaluate(self, dataloader, device):\n",
        "#         self.model.eval()\n",
        "#         total_loss = 0\n",
        "#         predictions, true_labels = [], []\n",
        "#         with torch.no_grad():\n",
        "#             for batch in tqdm(dataloader):\n",
        "#                 input_ids = batch['input_ids'].to(device)\n",
        "#                 attention_mask = batch['attention_mask'].to(device)\n",
        "#                 labels = batch['aspect_labels'].to(device)\n",
        "\n",
        "#                 outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#                 loss = outputs.loss\n",
        "#                 total_loss += loss.item()\n",
        "#                 logits = outputs.logits.argmax(dim=-1)\n",
        "#                 predictions.extend(logits.cpu().numpy())\n",
        "#                 true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "#         print(\"Evaluate - Classification Report:\")\n",
        "#         print(classification_report(true_labels, predictions))\n",
        "#         return total_loss / len(dataloader)\n"
      ],
      "metadata": {
        "id": "xvzo7-zJGiRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class SentimentClassifier:\n",
        "#     def __init__(self, num_labels):\n",
        "#         self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#         self.model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "#         self.model.train()\n",
        "\n",
        "#     def train(self, dataloader, device, epochs=3, lr=2e-5):\n",
        "#         optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "#         self.model.to(device)\n",
        "#         predictions, true_labels = [], []\n",
        "#         for epoch in range(epochs):\n",
        "#             total_loss = 0\n",
        "#             self.model.train()\n",
        "#             for batch in tqdm(dataloader):\n",
        "#                 input_ids = batch['input_ids'].to(device)\n",
        "#                 attention_mask = batch['attention_mask'].to(device)\n",
        "#                 sentiment_labels = batch['sentiment_labels'].to(device)\n",
        "\n",
        "#                 optimizer.zero_grad()\n",
        "#                 outputs = self.model(input_ids, attention_mask=attention_mask, labels=sentiment_labels)\n",
        "#                 loss = outputs.loss\n",
        "#                 loss.backward()\n",
        "#                 optimizer.step()\n",
        "#                 total_loss += loss.item()\n",
        "#                 logits = outputs.logits.argmax(dim=-1)\n",
        "#                 predictions.extend(logits.cpu().numpy())\n",
        "#                 true_labels.extend(sentiment_labels.cpu().numpy())\n",
        "\n",
        "#             print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "#         print(\"Train - Token-Level Sentiment Classification Report:\")\n",
        "#         print(classification_report(true_labels.flatten(), predictions.flatten(), labels=np.unique(predictions)))\n",
        "#         return total_loss / len(dataloader)\n",
        "\n",
        "#     def evaluate(self, dataloader, device):\n",
        "#         self.model.eval()\n",
        "#         total_loss = 0\n",
        "#         predictions, true_labels = [], []\n",
        "#         with torch.no_grad():\n",
        "#             for batch in tqdm(dataloader):\n",
        "#                 input_ids = batch['input_ids'].to(device)\n",
        "#                 attention_mask = batch['attention_mask'].to(device)\n",
        "#                 sentiment_labels = batch['sentiment_labels'].to(device)\n",
        "\n",
        "#                 outputs = self.model(input_ids, attention_mask=attention_mask, labels=sentiment_labels)\n",
        "#                 loss = outputs.loss\n",
        "#                 total_loss += loss.item()\n",
        "#                 logits = outputs.logits.argmax(dim=-1)\n",
        "#                 predictions.extend(logits.cpu().numpy())\n",
        "#                 true_labels.extend(sentiment_labels.cpu().numpy())\n",
        "\n",
        "#         print(\"Evaluate - Token-Level Sentiment Classification Report:\")\n",
        "#         print(classification_report(true_labels.flatten(), predictions.flatten(), labels=np.unique(predictions)))\n",
        "#         return total_loss / len(dataloader)\n"
      ],
      "metadata": {
        "id": "Ez94s24lO8tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "# # train_dataset = AspectSentimentDataset(train_df)\n",
        "# # test_dataset = AspectSentimentDataset(test_df)\n",
        "\n",
        "# # Instantiate models\n",
        "# aspect_model = AspectExtractor(num_labels=get_label_num('aspect_labels'))  # Adapt num_labels_aspect as per your dataset\n",
        "# sentiment_model = SentimentClassifier(num_labels=get_label_num('sentiment_labels'))  # Adapt num_labels_sentiment as per your dataset\n",
        "\n",
        "# # Device configuration\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # Train and evaluate sentiment model\n",
        "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "# sentiment_model.train(train_loader, device)\n",
        "# sentiment_model.evaluate(test_loader, device)\n"
      ],
      "metadata": {
        "id": "fXCU8v7-SqN4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}