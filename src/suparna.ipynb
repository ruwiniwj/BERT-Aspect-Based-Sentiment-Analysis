{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch\n",
    "!pip3 install pandas\n",
    "!pip3 install numpy\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scikit-learn\n",
    "!pip3 install transformers\n",
    "!pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(1, '../dataset')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from utils import tag_to_word_df\n",
    "import torch\n",
    "import argparse\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the ATE model with the testing data, this process may require some modifications later, this is to check if the trained model works for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def replace_sentiment_tags (tags):\n",
    "#     return [-1 if tag == 'O' else 2 if tag.endswith('positive') else 0 if tag.endswith('negative') else 1 if tag.endswith('neutral') else tag for tag in tags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--batch', type=int, default=8, help='batch size')\n",
    "# parser.add_argument('--epochs', type=int, default=5, help='number of epochs')\n",
    "# parser.add_argument('--lr', type=float, default=3*1e-5, help='learning rate')\n",
    "# parser.add_argument('--lr_schedule', type=bool, default=True, help='learning rate scheduler')\n",
    "# parser.add_argument('--adapter', type=bool, default=False, help='adapter')\n",
    "batch = 8\n",
    "epochs = 5\n",
    "lr = 3*1e-5\n",
    "lr_schedule = True\n",
    "adapter = False\n",
    "\n",
    "def train_ABTE (batch, epochs, lr, lr_schedule, adapter):\n",
    "\n",
    "    #load\n",
    "    data = pd.read_csv('../dataset/df_label_539980_train_20240705.csv')\n",
    "\n",
    "    data['bio_tags'] = data['bio_tags'].apply(utils.convert_to_array)\n",
    "    data['sentiment_tags'] = data['sentiment_tags'].apply(utils.convert_to_array)\n",
    "    data['tokens'] = data['tokens'].apply(utils.convert_to_array)\n",
    "\n",
    "    data['bio_tags'] = data['bio_tags'].apply(utils.replace_SEP).apply(utils.replace_tags)\n",
    "    data['sentiment_tags'] = data['sentiment_tags'].apply(utils.replace_SEP).apply(utils.replace_sentiment_tags)\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # if torch.backends.mps.is_available():\n",
    "    #     DEVICE = torch.device(\"mps\")\n",
    "    # else:\n",
    "    #     DEVICE = torch.device(\"cpu\")\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    from abte import ABTEModel\n",
    "    modelABTE = ABTEModel(tokenizer, adapter)\n",
    "    modelABTE.train(data, batch_size=batch, lr=lr, epochs=epochs, device=DEVICE, lr_schedule=lr_schedule)\n",
    "\n",
    "train_ABTE(batch, epochs, lr, lr_schedule, adapter)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abte import ABTEModel\n",
    "from utils import convert_to_array,replace_SEP,replace_tags, replace_sentiment_tags\n",
    "\n",
    "batch = 8\n",
    "lr = 3*1e-5\n",
    "epochs = 5\n",
    "\n",
    "def run_ABTE_test_train(adapter, lr_schedule):\n",
    "    if adapter:\n",
    "        if lr_schedule: dir_name  = \"model_ABTE_adapter_scheduler\"\n",
    "        else: dir_name = \"model_ABTE_adapter\"\n",
    "    else:\n",
    "        if lr_schedule: dir_name  = \"model_ABTE_scheduler\"\n",
    "        else: dir_name = \"model_ABTE\"\n",
    "    #load\n",
    "    data = pd.read_csv('../dataset/df_label_539980_train_20240705.csv')\n",
    "    data_test = pd.read_csv('../dataset/df_label_539980_test_20240705.csv')\n",
    "\n",
    "    data['bio_tags'] = data['bio_tags'].apply(convert_to_array)\n",
    "    data['sentiment_tags'] = data['sentiment_tags'].apply(convert_to_array)\n",
    "    data['tokens'] = data['tokens'].apply(convert_to_array)\n",
    "\n",
    "    data['bio_tags'] = data['bio_tags'].apply(replace_SEP).apply(replace_tags)\n",
    "    data['sentiment_tags'] = data['sentiment_tags'].apply(replace_SEP).apply(replace_sentiment_tags)\n",
    "\n",
    "    data_test['bio_tags'] = data_test['bio_tags'].apply(convert_to_array)\n",
    "    data_test['sentiment_tags'] = data_test['sentiment_tags'].apply(convert_to_array)\n",
    "    data_test['tokens'] = data_test['tokens'].apply(convert_to_array)\n",
    "\n",
    "    data_test['bio_tags'] = data_test['bio_tags'].apply(replace_SEP).apply(replace_tags)\n",
    "    data_test['sentiment_tags'] = data_test['sentiment_tags'].apply(replace_SEP).apply(replace_sentiment_tags)\n",
    "\n",
    "    # define parameters for model\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    # define model\n",
    "    modelABTE = ABTEModel(tokenizer, adapter=adapter)\n",
    "\n",
    "    # load model and predict\n",
    "    model_path = dir_name+'/model_lr3.0000000000000004e-05_epochs4_batch8.pkl'\n",
    "    test_accuracy, test_report = modelABTE.test(data_test, load_model=model_path, device=DEVICE)\n",
    "    test_pred, test_targets = modelABTE.predict_batch(data_test, load_model=model_path, device=DEVICE)\n",
    "\n",
    "    train_accuracy, train_report = modelABTE.test(data, load_model=model_path, device=DEVICE)\n",
    "    train_pred, train_targets = modelABTE.predict_batch(data, load_model=model_path, device=DEVICE)\n",
    "\n",
    "    print('*****test_report')\n",
    "    print(test_report)\n",
    "    print('****train_report')\n",
    "    print(train_report)\n",
    "\n",
    "\n",
    "    #save results\n",
    "    if not os.path.exists(dir_name+'/results'):\n",
    "        os.makedirs(dir_name+'/results')\n",
    "        print('folder not exists, created result folder')\n",
    "\n",
    "    #report\n",
    "    with open(dir_name+'/results/test_report_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        for r in test_report.split('\\n'):\n",
    "            f.write(r + '\\n')\n",
    "\n",
    "    with open(dir_name+'/results/train_report_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        for r in train_report.split('\\n'):\n",
    "            f.write(r + '\\n')\n",
    "\n",
    "    #predictions\n",
    "    data_test['Predicted'] = test_pred\n",
    "    data_test['Actual'] = test_targets\n",
    "    data_test.to_csv(dir_name+'/results/test_pred_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), index=False)\n",
    "\n",
    "    data['Predicted'] = train_pred\n",
    "    data['Actual'] = train_targets\n",
    "    data.to_csv(dir_name+'/results/train_pred_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), index=False)\n",
    "\n",
    "    #accuracy\n",
    "    test_accuracy = np.array(test_accuracy)\n",
    "    train_accuracy = np.array(train_accuracy)\n",
    "\n",
    "    with open(dir_name+'/results/test_accuracy_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        f.write(str(test_accuracy))\n",
    "    with open(dir_name+'/results/train_accuracy_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        f.write(str(train_accuracy))\n",
    "    \n",
    "    print('completed!!!!! huurayyyyyy!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "# DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# model_path = 'model_ABTE_scheduler/model_lr3.0000000000000004e-05_epochs4_batch8.pkl'\n",
    "\n",
    "# # define model\n",
    "# modelABTE = ABTEModel(tokenizer, False)\n",
    "# word_pieces, predictions, outputs = modelABTE.predict('It was a little tighter in the chest area than i thought it would be, but i still like it very much!', load_model=model_path, device=DEVICE)\n",
    "\n",
    "# print(word_pieces)\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ABTE_test_train(False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****test_report\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#         none       0.99      0.99      0.99      1883\n",
    "#  start of AT       0.74      0.71      0.72        55\n",
    "#   mark of AT       0.00      0.00      0.00         0\n",
    "\n",
    "#    micro avg       0.98      0.98      0.98      1938\n",
    "#    macro avg       0.58      0.57      0.57      1938\n",
    "# weighted avg       0.98      0.98      0.98      1938\n",
    "\n",
    "# ****train_report\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#         none       1.00      1.00      1.00      7564\n",
    "#  start of AT       0.91      0.89      0.90       224\n",
    "#   mark of AT       0.00      0.00      0.00         0\n",
    "\n",
    "#    micro avg       0.99      0.99      0.99      7788\n",
    "#    macro avg       0.64      0.63      0.63      7788\n",
    "# weighted avg       0.99      0.99      0.99      7788\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "    # define model\n",
    "    # load model and predict\n",
    "model_path = 'model_ABTE_scheduler'+'/model_lr3.0000000000000004e-05_epochs4_batch8.pkl'\n",
    "    \n",
    "# DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# model_path = 'model_ABTE_scheduler/model_lr3.0000000000000004e-05_epochs4_batch8.pkl'\n",
    "\n",
    "# define model\n",
    "modelABTE = ABTEModel(tokenizer, False)\n",
    "word_pieces, predictions, outputs = modelABTE.predict('It was a little tighter in the chest area than i thought it would be, but i still like it very much!', load_model=model_path, device=DEVICE)\n",
    "\n",
    "print(word_pieces)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelABTE = ABTEModel(tokenizer, False)\n",
    "# %time train_model_ATE(train_loader, 3)\n",
    "# model_ATE = load_model(model_ATE, 'bert_ATE.pkl')\n",
    "# %time x, y = test_model_ATE(test_loader)\n",
    "# print(classification_report(x, y, target_names=[str(i) for i in range(3)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Learning rate scheduler:  True\n",
      "Adapter:  True\n",
      "epoch: 0\tbatch: 1/16\tloss: 1.1772578954696655\tbatch time: 1.442\ttotal time: 1.442\n",
      "epoch: 0\tbatch: 2/16\tloss: 1.107589840888977\tbatch time: 1.59\ttotal time: 3.032\n",
      "epoch: 0\tbatch: 3/16\tloss: 1.1904220581054688\tbatch time: 1.251\ttotal time: 4.2829999999999995\n",
      "epoch: 0\tbatch: 4/16\tloss: 1.185410976409912\tbatch time: 0.611\ttotal time: 4.893999999999999\n",
      "epoch: 0\tbatch: 5/16\tloss: 1.1009628772735596\tbatch time: 0.635\ttotal time: 5.528999999999999\n",
      "epoch: 0\tbatch: 6/16\tloss: 1.1719692945480347\tbatch time: 0.579\ttotal time: 6.107999999999999\n",
      "epoch: 0\tbatch: 7/16\tloss: 1.2508803606033325\tbatch time: 0.661\ttotal time: 6.768999999999998\n",
      "epoch: 0\tbatch: 8/16\tloss: 1.091991662979126\tbatch time: 0.617\ttotal time: 7.385999999999998\n",
      "epoch: 0\tbatch: 9/16\tloss: 1.1399116516113281\tbatch time: 0.785\ttotal time: 8.170999999999998\n",
      "epoch: 0\tbatch: 10/16\tloss: 1.2514110803604126\tbatch time: 1.397\ttotal time: 9.567999999999998\n",
      "epoch: 0\tbatch: 11/16\tloss: 1.0176470279693604\tbatch time: 0.571\ttotal time: 10.138999999999998\n",
      "epoch: 0\tbatch: 12/16\tloss: 1.1092016696929932\tbatch time: 0.725\ttotal time: 10.863999999999997\n",
      "epoch: 0\tbatch: 13/16\tloss: 0.9669245481491089\tbatch time: 0.545\ttotal time: 11.408999999999997\n",
      "epoch: 0\tbatch: 14/16\tloss: 0.959587812423706\tbatch time: 0.637\ttotal time: 12.045999999999998\n",
      "epoch: 0\tbatch: 15/16\tloss: 0.8628150224685669\tbatch time: 0.683\ttotal time: 12.728999999999997\n",
      "epoch: 0\tbatch: 16/16\tloss: 0.9308062195777893\tbatch time: 0.751\ttotal time: 13.479999999999997\n",
      "epoch: 1\tbatch: 1/16\tloss: 0.9857332706451416\tbatch time: 0.857\ttotal time: 0.857\n",
      "epoch: 1\tbatch: 2/16\tloss: 1.1017343997955322\tbatch time: 0.513\ttotal time: 1.37\n",
      "epoch: 1\tbatch: 3/16\tloss: 1.1700496673583984\tbatch time: 0.734\ttotal time: 2.104\n",
      "epoch: 1\tbatch: 4/16\tloss: 0.7604926824569702\tbatch time: 0.58\ttotal time: 2.684\n",
      "epoch: 1\tbatch: 5/16\tloss: 0.733086884021759\tbatch time: 0.669\ttotal time: 3.353\n",
      "epoch: 1\tbatch: 6/16\tloss: 1.0029993057250977\tbatch time: 0.618\ttotal time: 3.971\n",
      "epoch: 1\tbatch: 7/16\tloss: 0.8947589993476868\tbatch time: 0.546\ttotal time: 4.517\n",
      "epoch: 1\tbatch: 8/16\tloss: 0.7832256555557251\tbatch time: 0.524\ttotal time: 5.041\n",
      "epoch: 1\tbatch: 9/16\tloss: 0.8028400540351868\tbatch time: 0.499\ttotal time: 5.54\n",
      "epoch: 1\tbatch: 10/16\tloss: 0.7995128035545349\tbatch time: 0.675\ttotal time: 6.215\n",
      "epoch: 1\tbatch: 11/16\tloss: 0.5580207705497742\tbatch time: 0.707\ttotal time: 6.922\n",
      "epoch: 1\tbatch: 12/16\tloss: 0.7144719362258911\tbatch time: 0.687\ttotal time: 7.609\n",
      "epoch: 1\tbatch: 13/16\tloss: 0.8725869655609131\tbatch time: 0.451\ttotal time: 8.06\n",
      "epoch: 1\tbatch: 14/16\tloss: 1.1909836530685425\tbatch time: 0.647\ttotal time: 8.707\n",
      "epoch: 1\tbatch: 15/16\tloss: 0.992798388004303\tbatch time: 0.859\ttotal time: 9.566\n",
      "epoch: 1\tbatch: 16/16\tloss: 0.6812797784805298\tbatch time: 0.58\ttotal time: 10.146\n",
      "epoch: 2\tbatch: 1/16\tloss: 0.7844353318214417\tbatch time: 0.601\ttotal time: 0.601\n",
      "epoch: 2\tbatch: 2/16\tloss: 0.49623140692710876\tbatch time: 0.592\ttotal time: 1.193\n",
      "epoch: 2\tbatch: 3/16\tloss: 0.6913179755210876\tbatch time: 0.667\ttotal time: 1.86\n",
      "epoch: 2\tbatch: 4/16\tloss: 0.5055484771728516\tbatch time: 0.642\ttotal time: 2.5020000000000002\n",
      "epoch: 2\tbatch: 5/16\tloss: 0.548224151134491\tbatch time: 0.588\ttotal time: 3.0900000000000003\n",
      "epoch: 2\tbatch: 6/16\tloss: 0.7893831729888916\tbatch time: 0.559\ttotal time: 3.6490000000000005\n",
      "epoch: 2\tbatch: 7/16\tloss: 0.8209367394447327\tbatch time: 0.684\ttotal time: 4.333\n",
      "epoch: 2\tbatch: 8/16\tloss: 0.6750484108924866\tbatch time: 0.694\ttotal time: 5.027\n",
      "epoch: 2\tbatch: 9/16\tloss: 0.7449250817298889\tbatch time: 0.532\ttotal time: 5.559\n",
      "epoch: 2\tbatch: 10/16\tloss: 0.5196715593338013\tbatch time: 0.541\ttotal time: 6.1000000000000005\n",
      "epoch: 2\tbatch: 11/16\tloss: 0.726604700088501\tbatch time: 0.641\ttotal time: 6.7410000000000005\n",
      "epoch: 2\tbatch: 12/16\tloss: 0.6255225539207458\tbatch time: 0.512\ttotal time: 7.253\n",
      "epoch: 2\tbatch: 13/16\tloss: 0.7539265751838684\tbatch time: 0.725\ttotal time: 7.978\n",
      "epoch: 2\tbatch: 14/16\tloss: 0.9023631811141968\tbatch time: 0.693\ttotal time: 8.671\n",
      "epoch: 2\tbatch: 15/16\tloss: 0.7272284626960754\tbatch time: 0.609\ttotal time: 9.28\n",
      "epoch: 2\tbatch: 16/16\tloss: 0.8246396780014038\tbatch time: 0.675\ttotal time: 9.955\n",
      "epoch: 3\tbatch: 1/16\tloss: 0.9423414468765259\tbatch time: 0.662\ttotal time: 0.662\n",
      "epoch: 3\tbatch: 2/16\tloss: 0.6688780188560486\tbatch time: 0.682\ttotal time: 1.344\n",
      "epoch: 3\tbatch: 3/16\tloss: 0.5527077317237854\tbatch time: 0.723\ttotal time: 2.067\n",
      "epoch: 3\tbatch: 4/16\tloss: 0.594856858253479\tbatch time: 0.6\ttotal time: 2.6670000000000003\n",
      "epoch: 3\tbatch: 5/16\tloss: 1.005379319190979\tbatch time: 0.671\ttotal time: 3.338\n",
      "epoch: 3\tbatch: 6/16\tloss: 0.6044788360595703\tbatch time: 0.585\ttotal time: 3.923\n",
      "epoch: 3\tbatch: 7/16\tloss: 0.8334593176841736\tbatch time: 0.726\ttotal time: 4.649\n",
      "epoch: 3\tbatch: 8/16\tloss: 0.41649889945983887\tbatch time: 0.522\ttotal time: 5.171\n",
      "epoch: 3\tbatch: 9/16\tloss: 0.4812406599521637\tbatch time: 0.589\ttotal time: 5.76\n",
      "epoch: 3\tbatch: 10/16\tloss: 0.5903654098510742\tbatch time: 0.671\ttotal time: 6.431\n",
      "epoch: 3\tbatch: 11/16\tloss: 0.4559915065765381\tbatch time: 0.606\ttotal time: 7.037\n",
      "epoch: 3\tbatch: 12/16\tloss: 0.500407874584198\tbatch time: 0.506\ttotal time: 7.543\n",
      "epoch: 3\tbatch: 13/16\tloss: 0.5076619386672974\tbatch time: 0.696\ttotal time: 8.239\n",
      "epoch: 3\tbatch: 14/16\tloss: 0.3833021819591522\tbatch time: 0.614\ttotal time: 8.853000000000002\n",
      "epoch: 3\tbatch: 15/16\tloss: 0.3588927388191223\tbatch time: 0.649\ttotal time: 9.502000000000002\n",
      "epoch: 3\tbatch: 16/16\tloss: 0.4899187684059143\tbatch time: 0.603\ttotal time: 10.105000000000002\n",
      "epoch: 4\tbatch: 1/16\tloss: 0.7347874641418457\tbatch time: 0.594\ttotal time: 0.594\n",
      "epoch: 4\tbatch: 2/16\tloss: 0.3857768476009369\tbatch time: 1.005\ttotal time: 1.5989999999999998\n",
      "epoch: 4\tbatch: 3/16\tloss: 0.37338709831237793\tbatch time: 0.932\ttotal time: 2.5309999999999997\n",
      "epoch: 4\tbatch: 4/16\tloss: 0.5217245817184448\tbatch time: 0.673\ttotal time: 3.2039999999999997\n",
      "epoch: 4\tbatch: 5/16\tloss: 0.4126988649368286\tbatch time: 0.775\ttotal time: 3.9789999999999996\n",
      "epoch: 4\tbatch: 6/16\tloss: 0.4369925558567047\tbatch time: 0.865\ttotal time: 4.843999999999999\n",
      "epoch: 4\tbatch: 7/16\tloss: 0.6120943427085876\tbatch time: 0.676\ttotal time: 5.52\n",
      "epoch: 4\tbatch: 8/16\tloss: 0.33508408069610596\tbatch time: 0.651\ttotal time: 6.170999999999999\n",
      "epoch: 4\tbatch: 9/16\tloss: 0.2024131417274475\tbatch time: 1.18\ttotal time: 7.350999999999999\n",
      "epoch: 4\tbatch: 10/16\tloss: 0.3270869851112366\tbatch time: 0.719\ttotal time: 8.069999999999999\n",
      "epoch: 4\tbatch: 11/16\tloss: 0.7983314990997314\tbatch time: 0.542\ttotal time: 8.611999999999998\n",
      "epoch: 4\tbatch: 12/16\tloss: 0.3613409399986267\tbatch time: 0.673\ttotal time: 9.284999999999998\n",
      "epoch: 4\tbatch: 13/16\tloss: 0.5265377163887024\tbatch time: 0.606\ttotal time: 9.890999999999998\n",
      "epoch: 4\tbatch: 14/16\tloss: 0.24202272295951843\tbatch time: 0.56\ttotal time: 10.450999999999999\n",
      "epoch: 4\tbatch: 15/16\tloss: 0.4244743287563324\tbatch time: 0.805\ttotal time: 11.255999999999998\n",
      "epoch: 4\tbatch: 16/16\tloss: 0.35286054015159607\tbatch time: 0.518\ttotal time: 11.774\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "batch = 8\n",
    "epochs = 5\n",
    "lr = 3*1e-5\n",
    "lr_schedule = True\n",
    "adapter = False\n",
    "\n",
    "def train_ABSA (batch, epochs, lr, lr_schedule, adapter):\n",
    "\n",
    "    #load\n",
    "    data = pd.read_csv('../dataset/df_label_539980_train_20240705.csv')\n",
    "\n",
    "    data['bio_tags'] = data['bio_tags'].apply(utils.convert_to_array)\n",
    "    data['sentiment_tags'] = data['sentiment_tags'].apply(utils.convert_to_array)\n",
    "    data['tokens'] = data['tokens'].apply(utils.convert_to_array)\n",
    "\n",
    "    data['bio_tags'] = data['bio_tags'].apply(utils.replace_SEP).apply(utils.replace_tags)\n",
    "    data['sentiment_tags'] = data['sentiment_tags'].apply(utils.replace_SEP).apply(utils.replace_sentiment_tags)\n",
    "\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    \n",
    "    from absa import ABSAModel\n",
    "    modelABSA = ABSAModel(tokenizer, adapter=True)\n",
    "    modelABSA.train(data, batch_size=batch, lr=lr, epochs=epochs, device=DEVICE, lr_schedule=True)\n",
    "\n",
    "train_ABSA(batch, epochs, lr, lr_schedule, adapter)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absa import ABSAModel\n",
    "from utils import convert_to_array,replace_SEP,replace_tags, replace_sentiment_tags\n",
    "\n",
    "#save results\n",
    "batch = 8\n",
    "lr = 1e-5\n",
    "epochs = 5\n",
    "\n",
    "def run_ABSA_test_train(adapter, lr_schedule):\n",
    "    if adapter:\n",
    "        if lr_schedule: dir_name_s  = \"model_ABSA_adapter_scheduler\"\n",
    "        else: dir_name_s = \"model_ABSA_adapter\"\n",
    "    else:\n",
    "        if lr_schedule: dir_name_s  = \"model_ABSA_scheduler\"\n",
    "        else: dir_name_s = \"model_ABSA\"\n",
    "    print(dir_name_s)\n",
    "\n",
    "    #load\n",
    "    data = pd.read_csv('../dataset/df_label_539980_train_20240705.csv')\n",
    "    data_test = pd.read_csv('../dataset/df_label_539980_test_20240705.csv')\n",
    "\n",
    "    data['bio_tags'] = data['bio_tags'].apply(convert_to_array)\n",
    "    data['sentiment_tags'] = data['sentiment_tags'].apply(convert_to_array)\n",
    "    data['tokens'] = data['tokens'].apply(convert_to_array)\n",
    "\n",
    "    data['bio_tags'] = data['bio_tags'].apply(replace_SEP).apply(replace_tags)\n",
    "    data['sentiment_tags'] = data['sentiment_tags'].apply(replace_SEP).apply(replace_sentiment_tags)\n",
    "\n",
    "    data_test['bio_tags'] = data_test['bio_tags'].apply(convert_to_array)\n",
    "    data_test['sentiment_tags'] = data_test['sentiment_tags'].apply(convert_to_array)\n",
    "    data_test['tokens'] = data_test['tokens'].apply(convert_to_array)\n",
    "\n",
    "    data_test['bio_tags'] = data_test['bio_tags'].apply(replace_SEP).apply(replace_tags)\n",
    "    data_test['sentiment_tags'] = data_test['sentiment_tags'].apply(replace_SEP).apply(replace_sentiment_tags)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "    modelABSA = ABSAModel(tokenizer, adapter=adapter)\n",
    "\n",
    "    model_path = dir_name_s+'/model_lr3.0000000000000004e-05_epochs4_batch8.pkl'\n",
    "    test_accuracy, test_report = modelABSA.test(data_test, load_model=model_path, device=DEVICE)\n",
    "    test_pred, test_pol = modelABSA.predict_batch(data_test, load_model=model_path, device=DEVICE)\n",
    "\n",
    "    train_accuracy, train_report = modelABSA.test(data, load_model=model_path, device=DEVICE)\n",
    "    train_pred, train_pol = modelABSA.predict_batch(data, load_model=model_path, device=DEVICE)\n",
    "\n",
    "    print('*****test_report')\n",
    "    print(test_report)\n",
    "    print('****train_report')\n",
    "    print(train_report)\n",
    "\n",
    "    #save results\n",
    "    if not os.path.exists(dir_name_s+'/results'):\n",
    "        os.makedirs(dir_name_s+'/results')\n",
    "\n",
    "    #report\n",
    "    with open(dir_name_s+'/results/test_report_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        for r in test_report.split('\\n'):\n",
    "            f.write(r + '\\n')\n",
    "\n",
    "    with open(dir_name_s+'/results/train_report_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        for r in train_report.split('\\n'):\n",
    "            f.write(r + '\\n')\n",
    "\n",
    "    #predictions\n",
    "    data_test['Predicted'] = test_pred\n",
    "    data_test['Actual'] = test_pol\n",
    "    data_test.to_csv(dir_name_s+'/results/test_pred_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), index=False)\n",
    "\n",
    "    data['Predicted'] = train_pred\n",
    "    data['Actual'] = train_pol\n",
    "    data.to_csv(dir_name_s+'/results/train_pred_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), index=False)\n",
    "\n",
    "    #accuracy\n",
    "    test_accuracy = np.array(test_accuracy)\n",
    "    train_accuracy = np.array(train_accuracy)\n",
    "\n",
    "    with open(dir_name_s+'/results/test_accuracy_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        f.write(str(test_accuracy))\n",
    "    with open(dir_name_s+'/results/train_accuracy_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        f.write(str(train_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_ABSA_adapter_scheduler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "100%|██████████| 34/34 [00:07<00:00,  4.75it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.18it/s]\n",
      "100%|██████████| 132/132 [00:29<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****test_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.71      0.70        21\n",
      "     neutral       0.00      0.00      0.00         1\n",
      "    positive       0.50      0.50      0.50        12\n",
      "\n",
      "    accuracy                           0.62        34\n",
      "   macro avg       0.39      0.40      0.40        34\n",
      "weighted avg       0.60      0.62      0.61        34\n",
      "\n",
      "****train_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.96        88\n",
      "     neutral       0.00      0.00      0.00         4\n",
      "    positive       0.95      0.88      0.91        40\n",
      "\n",
      "    accuracy                           0.93       132\n",
      "   macro avg       0.62      0.62      0.62       132\n",
      "weighted avg       0.90      0.93      0.92       132\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_ABSA_test_train(True, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eff73bd6cd3d0fe52265b8b7ef995f74a43a48c078b1368b9c809f395965f11b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
